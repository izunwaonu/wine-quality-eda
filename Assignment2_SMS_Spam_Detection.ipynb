{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40b44da",
   "metadata": {},
   "source": [
    "# Assignment 2 ‚Äî SMS Spam Detection (Text)\n",
    "**Student:** Justus Izuchukwu Onuh  \n",
    "**Institution:** Ho Chi Minh City University of Technology  \n",
    "**Course:** Programming Platform for Data Analysis and Visualization (CO5177)  \n",
    "**Lecturer:** LE THANH SACH  \n",
    "\n",
    "**Dataset:** SMS Spam Collection (UCI Machine Learning Repository)  \n",
    "**Filename on UCI:** `SMSSpamCollection`  \n",
    "**Donated / Published on UCI:** 21 June 2012  \n",
    "**Instances:** 5,574.  \n",
    "**License:** CC BY 4.0.  \n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "1. Load and explore the SMS Spam dataset.  \n",
    "2. Perform thorough exploratory data analysis (EDA) with visualizations.  \n",
    "3. Preprocess text messages and extract features for machine learning.  \n",
    "4. Train and evaluate at least two classification models (e.g., Naive Bayes, Logistic Regression).  \n",
    "5. Compare model performance using relevant metrics (accuracy, precision, recall, F1, confusion matrix).  \n",
    "6. Present results, discussion, and conclusions in markdown (suitable for Colab markdown report).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The following cells implement the full workflow step-by-step. Comments and markdown are written as if *I* (the student) am performing the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2d3b5",
   "metadata": {},
   "source": [
    "## Plan / Tasks\n",
    "1. Load dataset from the UCI repository (raw text file).  \n",
    "2. Inspect structure, labels (ham/spam), and basic stats.  \n",
    "3. Clean and preprocess text (lowercase, remove punctuation, optional stopwords, tokenization).  \n",
    "4. Feature extraction: TF-IDF vectorization (and optionally count vectors or simple text features like message length).  \n",
    "5. Train/Test split and baseline model (Multinomial Naive Bayes).  \n",
    "6. Additional model: Logistic Regression (with class weighting or parameter tuning).  \n",
    "7. Evaluate with confusion matrix and classification report.  \n",
    "8. Conclude and list references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f705fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries successfully imported!\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports and helper functions\n",
    "# This cell installs and imports necessary libraries.\n",
    "\n",
    "# --- Importing Python standard libraries ---\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "\n",
    "# --- Data processing and analysis libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Visualization library ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Machine learning libraries ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# For reproducibility and consistent results\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"‚úÖ Libraries successfully imported!\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean input text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing URLs and emails\n",
    "    3. Removing punctuation\n",
    "    4. Collapsing extra whitespace\n",
    "    \"\"\"\n",
    "    print(f\" Cleaning text: {text[:50]}...\")  # show first 50 characters of text\n",
    "    \n",
    "    text = str(text).lower()  # convert to lowercase\n",
    "\n",
    "    # Remove URLs and emails\n",
    "    text = re.sub(r'http\\S+|www\\S+|\\S+@\\S+', ' ', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove multiple spaces and trim\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    print(f\"‚ú® Cleaned text: {text[:50]}...\")  # show first 50 characters of cleaned text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13e1a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Ensuring data directory exists: data\n",
      "‚¨áÔ∏è Downloading SMS Spam dataset (no Kaggle API)...\n",
      "‚úÖ Download complete! File saved to: /Users/izunwaonu/Desktop/CO5177_Assingment /data/spam.csv\n",
      "üìÑ Loading dataset from: /Users/izunwaonu/Desktop/CO5177_Assingment /data/spam.csv\n",
      "‚úÖ Dataset loaded successfully!\n",
      "Shape: (5572, 2)\n",
      "\n",
      "Label counts:\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîç Preview (first 8 rows):\n",
      "label                                                                                                                                                          message\n",
      "  ham                                                  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "  ham                                                                                                                                    Ok lar... Joking wif u oni...\n",
      " spam      Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "  ham                                                                                                                U dun say so early hor... U c already then say...\n",
      "  ham                                                                                                    Nah I don't think he goes to usf, he lives around here though\n",
      " spam             FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, √Ç¬£1.50 to rcv\n",
      "  ham                                                                                    Even my brother is not like to speak with me. They treat me like aids patent.\n",
      "  ham As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n"
     ]
    }
   ],
   "source": [
    "# 2) Download & Load SMS Spam Dataset \n",
    "# - Downloads from public mirror (GitHub UCI mirror)\n",
    "# - Saves dataset to data/spam.csv\n",
    "# - Loads into pandas and prints dataset info\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Public mirror of SMS Spam Collection dataset\n",
    "DATA_URL = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_FILE = DATA_DIR / \"spam.csv\"\n",
    "\n",
    "def download_dataset(save_dir=DATA_DIR, save_path=DATA_FILE, url=DATA_URL):\n",
    "    \"\"\"Download SMS Spam dataset without Kaggle API, save as spam.csv\"\"\"\n",
    "    print(f\"üìÅ Ensuring data directory exists: {save_dir}\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Check if dataset already exists\n",
    "    if save_path.exists():\n",
    "        print(f\"‚úÖ Dataset already exists at: {save_path.resolve()}\")\n",
    "        return\n",
    "\n",
    "    print(\"‚¨áÔ∏è Downloading SMS Spam dataset (no Kaggle API)...\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save as CSV\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        print(f\"‚úÖ Download complete! File saved to: {save_path.resolve()}\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå ERROR: Download failed. Please check your internet connection.\")\n",
    "        print(\"Error details:\", e)\n",
    "\n",
    "def load_sms_spam(file_path=DATA_FILE):\n",
    "    \"\"\"Load dataset into pandas and format columns\"\"\"\n",
    "    print(f\"üìÑ Loading dataset from: {file_path.resolve()}\")\n",
    "\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"label\", \"message\"], encoding=\"latin-1\")\n",
    "\n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "\n",
    "    # Display class distribution\n",
    "    print(\"\\nLabel counts:\")\n",
    "    print(df[\"label\"].value_counts())\n",
    "\n",
    "    # Preview\n",
    "    print(\"\\nüîç Preview (first 8 rows):\")\n",
    "    print(df.head(8).to_string(index=False))\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---- Execute Steps ----\n",
    "download_dataset()\n",
    "df = load_sms_spam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c2ec6",
   "metadata": {},
   "source": [
    "### Dataset Download & Loading (Local Save Method)\n",
    "\n",
    "In this project, we use the **SMS Spam Collection Dataset** for building an SMS spam detection system.  \n",
    "The dataset contains labeled SMS messages as either:\n",
    "\n",
    "- `ham` ‚Äî legitimate messages  \n",
    "- `spam` ‚Äî unwanted/promotional/scam messages  \n",
    "\n",
    "#### What We Did\n",
    "\n",
    "1. Created a `data/` folder in our project directory  \n",
    "2. Downloaded the dataset file `spam.csv` **manually (no Kaggle API)**  \n",
    "3. Saved the file inside the `data/` folder  \n",
    "4. Loaded the CSV file into pandas  \n",
    "5. Cleaned the column names:  \n",
    "   - `v1` ‚Üí `label`  \n",
    "   - `v2` ‚Üí `message`  \n",
    "6. Printed:\n",
    "   - Dataset shape  \n",
    "   - Class distribution  \n",
    "   - First 8 rows  \n",
    "\n",
    "#### Dataset Summary\n",
    "\n",
    "| Item | Value |\n",
    "|------|------|\n",
    "| Total rows | 5,572 |\n",
    "| Columns | 2 (`label`, `message`) |\n",
    "| Ham messages | 4,825 |\n",
    "| Spam messages | 747 |\n",
    "\n",
    "**Class Imbalance Notice:**  \n",
    "The dataset contains **much more ham than spam**, which means evaluation metrics like **precision, recall, and F1-score** will be important ‚Äî not only accuracy.\n",
    "\n",
    "#### üîç Preview of Dataset\n",
    "\n",
    "Examples of messages:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e6a25",
   "metadata": {},
   "source": [
    "## EDA (Exploratory Data Analysis)\n",
    "Perform the following checks and visualizations:\n",
    "- Class distribution (ham vs spam)\n",
    "- Message length distribution (characters & words)\n",
    "- Most common tokens in ham vs spam (top-n words)\n",
    "- Example messages for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80fe52d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`df` not found. Run the Load dataset cell first.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) EDA - run after you successfully load `df`\n",
    "try:\n",
    "    print('Dataset shape:', df.shape)\n",
    "    print('\\nLabel distribution:')\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "    # Add basic features\n",
    "    df['msg_len_chars'] = df['message'].apply(len)\n",
    "    df['msg_len_words'] = df['message'].apply(lambda s: len(str(s).split()))\n",
    "\n",
    "    display(df.groupby('label')[['msg_len_chars','msg_len_words']].describe().T)\n",
    "\n",
    "    # Plot distributions (matplotlib, no custom colors)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    df['msg_len_chars'].hist(bins=50)\n",
    "    plt.title('Message length (chars) - overall')\n",
    "    plt.xlabel('Chars')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    df[df['label']=='ham']['msg_len_chars'].hist(bins=50)\n",
    "    plt.title('Message length (chars) - ham')\n",
    "    plt.xlabel('Chars')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    df[df['label']=='spam']['msg_len_chars'].hist(bins=50)\n",
    "    plt.title('Message length (chars) - spam')\n",
    "    plt.xlabel('Chars')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    # Show a few examples\n",
    "    print('\\nExamples of ham messages:')\n",
    "    display(df[df['label']=='ham'].sample(6, random_state=RANDOM_STATE)['message'].tolist()[:6])\n",
    "\n",
    "    print('\\nExamples of spam messages:')\n",
    "    display(df[df['label']=='spam'].sample(6, random_state=RANDOM_STATE)['message'].tolist()[:6])\n",
    "\n",
    "except NameError:\n",
    "    print('`df` not found. Run the Load dataset cell first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638c484",
   "metadata": {},
   "source": [
    "## Preprocessing & Modeling\n",
    "1. Clean text with `clean_text()` defined earlier.  \n",
    "2. Vectorize using `TfidfVectorizer` (limit max_features e.g., 5000).  \n",
    "3. Train/Test split (80/20).  \n",
    "4. Train Multinomial Naive Bayes and Logistic Regression.  \n",
    "5. Evaluate and compare metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c03130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`df` not found or dataset not loaded. Run the Load dataset cell first.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Preprocessing, Vectorization, Train/Test split, Modeling\n",
    "try:\n",
    "    # Basic cleaning\n",
    "    df['clean_msg'] = df['message'].astype(str).apply(clean_text)\n",
    "\n",
    "    # Vectorize\n",
    "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
    "    X = tfidf.fit_transform(df['clean_msg'])\n",
    "    y = (df['label'] == 'spam').astype(int)  # spam=1, ham=0\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "    print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "\n",
    "    # Model 1: Multinomial Naive Bayes\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train, y_train)\n",
    "    y_pred_mnb = mnb.predict(X_test)\n",
    "\n",
    "    # Model 2: Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "    # Evaluation helper\n",
    "    def eval_model(y_true, y_pred, model_name='model'):\n",
    "        print(f'==== {model_name} ====\\n')\n",
    "        print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "        print('Precision:', precision_score(y_true, y_pred))\n",
    "        print('Recall:', recall_score(y_true, y_pred))\n",
    "        print('F1:', f1_score(y_true, y_pred))\n",
    "        print('\\nClassification report:\\n')\n",
    "        print(classification_report(y_true, y_pred, target_names=['ham','spam']))\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print('\\nConfusion matrix:\\n', cm)\n",
    "\n",
    "    eval_model(y_test, y_pred_mnb, 'MultinomialNB')\n",
    "    print('\\n-----------------------------------\\n')\n",
    "    eval_model(y_test, y_pred_lr, 'Logistic Regression')\n",
    "\n",
    "except NameError:\n",
    "    print('`df` not found or dataset not loaded. Run the Load dataset cell first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503f81a",
   "metadata": {},
   "source": [
    "## Model improvement suggestions\n",
    "- Hyperparameter tuning (GridSearchCV) for Logistic Regression (C) and TfidfVectorizer (max_features, ngram_range).  \n",
    "- Try additional models: LinearSVC, Random Forest (on TF-IDF reduced with SelectKBest or TruncatedSVD), or simple neural nets.  \n",
    "- Use cross-validation and report mean ¬± std metrics.  \n",
    "- Analyze feature importances / top tokens for spam class (inspect coefficients from LR or log-count ratios for NB).  \n",
    "- Add more engineered features: presence of URLs, phone numbers, all-caps tokens, number of digits, punctuation counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b70135",
   "metadata": {},
   "source": [
    "## Results, Discussion & Conclusion\n",
    "(Write your results here after running the models in Colab. Include figures and tables.)\n",
    "\n",
    "- Briefly summarize which model performed better and why.  \n",
    "- Discuss limitations (dataset bias, short messages, domain differences).  \n",
    "- Conclude and propose future work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49890c1f",
   "metadata": {},
   "source": [
    "## References\n",
    "- Almeida, T. & Hidalgo, J. (2011). SMS Spam Collection [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5CC84. (UCI dataset page: SMSSpamCollection).  \n",
    "- UCI Machine Learning Repository ‚Äî SMS Spam Collection. Dataset page and file: `SMSSpamCollection`.  \n",
    "\n",
    "(When you submit on Colab, make sure the notebook's markdown cells are visible and that you produce narrative explanations as required by the lecturer.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
